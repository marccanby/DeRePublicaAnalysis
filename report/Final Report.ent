\@doanenote {1}
macro:->All
of
my
code,
as
well
as
other
information,
can
be
found
at
\cite
{code}.
\@endanenote 
\@doanenote {2}
macro:->I
used
the
Classical
Languages
Toolkit
(CLTK),
a
Python
library
with
support
for
loading
classical
texts,
performing
morphological
analyses,
and
running
various
algorithms
\cite
{cltk}.
\@endanenote 
\@doanenote {3}
macro:->I
used
the
works
that
\textit
{The
Latin
Library}
deems
to
be
Cicero's
philosophical
works
for
this
analysis.
A
listing
can
be
found
at
\cite
{drp}.
\@endanenote 
\@doanenote {4}
macro:->I
used
pre-built
functionality
from
the
CLTK
to
determine
parts
of
speech;
I
did
not
code
a
part-of-speech
tagger
from
scratch
\cite
{cltk}.
I
also
used
the
{\ttfamily
ggplot}
package
in
the
R
coding
language
to
produce
these
plots
\cite
{ggplot}.
\@endanenote 
\@doanenote {5}
macro:->Algorithms
to
identify
stop
words
and
\textit
{lemmatize}
words
(find
the
base
forms
of
inflected
words)
are
provided
by
the
CLTK
\cite
{cltk}.
\@endanenote 
\@doanenote {6}
macro:->This
is
called
the
Continuous
Bag
of
Words
(CBOW)
architecture;
another
common
architecture
for
Word2Vec
is
the
Skip-gram.
A
discussion
of
the
difference
between
the
architectures
can
be
found
at
\cite
{pythonnlp}.
I
obtained
better
results
on
the
CBOW
architecture.
\@endanenote 
\@doanenote {7}
macro:->I
used
the
$t$-Distributed
Stochastic
Neighbor
Embedding
(t-SNE)
algorithm
for
dimensionality
reduction.
I
also
attempted
Principle
Component
Analysis
(PCA)
and
Independent
Component
Analysis
(ICA),
but
had
best
results
with
$t$-SNE
\cite
{dimred}.
\@endanenote 
\@doanenote {8}
macro:->Readers
interested
in
TFIDF
vectors
can
find
more
information
at
\cite
{pythonnlp}.
\@endanenote 
\@doanenote {9}
macro:->Three
sentences
were
selected
at
random
from
each
group.
\@endanenote 
\@doanenote {10}
macro:->The
text
with
each
sentence
highlighted
by
group
membership
can
be
found
at
\cite
{code}.
\@endanenote 
