{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De Re Publica Analysis\n",
    "\n",
    "This notebook contains some preliminary investigations of Cicero's De Re Publica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Read and clean in the data\n",
    "\n",
    "First we read in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 832\n",
      "Number of words: 24885\n"
     ]
    }
   ],
   "source": [
    "from cltk.corpus.latin import latinlibrary\n",
    "files = latinlibrary.fileids()\n",
    "drp_files = [f for f in files if 'cicero/repub' in f]\n",
    "\n",
    "drp_raw = latinlibrary.raw(drp_files)\n",
    "drp_sents = latinlibrary.sents(drp_files)\n",
    "drp_words = latinlibrary.words(drp_files)\n",
    "\n",
    "print(\"Number of sentences:\", len(drp_sents))\n",
    "print(\"Number of words:\", len(drp_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with the sentence data. We will clean it up to remove punctuation and non-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 : ['cicero', 'reor', 'publico', 'eo']\n",
      "Sentence 1 : ['tulli', 'ciceronis', 'reor', 'publico', 'libo', 'primus']\n",
      "Sentence 2 : ['petu', 'liberauissent', 'duelius', 'atilius', 'metellus', 'terror', 'karthaginis', 'duo', 'scipio', 'orior', 'incendium', 'bellus', 'punici', 'secundus', 'sanguis', 'suo', 'restinguo', 'excito', 'magnus', 'copia', 'magnus', 'eneruauisset', 'marcellus', 'contundo', 'porta', 'urbs', 'auolsum', 'africanus', 'compello', 'intro', 'hostis', 'moenia']\n",
      "Sentence 3 : ['verus', 'catonium', 'homo', 'ignosco', 'novo', 'omnes', 'res', 'studeo', 'quasi', 'exemplo', 'industria', 'virtus', 'duco', 'certus', 'liceo', 'tusculum', 'otium', 'delecto', 'salubris', 'propinquo', 'loco']\n",
      "Sentence 4 : ['homo', 'demo', 'iste', 'puto', 'cogo', 'necessitas', 'nullus', 'unda', 'tempestas', 'summa', 'senectus', 'malo', 'jacto', 'tranquillitas', 'otium', 'jucundus', 'vivo']\n",
      "Sentence 5 : ['omitto', 'innumerabilis', 'vir', 'singulus', 'salus', 'civitas', 'procul', 'aetas', 'memor', 'commemoro', 'desino', 'neo', 'queo', 'suum', 'praetermitto', 'queror']\n",
      "Sentence 6 : ['definio', 'tantus', 'edo', 'necessitas', 'virtus', 'gener', 'homo', 'natura', 'tantus', 'amor', 'communis', 'saluto', 'defendo', 'do', 'volo', 'omne', 'blandimentum', 'voluptas', 'otium', 'vinco']\n",
      "Sentence 7 : ['verus', 'habeo', 'virtus', 'sero', 'edo', 'quasi', 'arto', 'nitor', 'uto', 'ars', 'uto', 'scio', 'teneo', 'virtus', 'utor', 'suo', 'totus', 'pono', 'edo', 'utor', 'edo', 'magnus', 'civitas', 'gubernatio', 'res', 'iste', 'angulus', 'persono', 'reapse', 'oratio', 'perfectio']\n",
      "Sentence 8 : ['nihil', 'dico', 'philosophus', 'recingo', 'honestus', 'dico', 'pario', 'confirmo', 'civitas', 'juro', 'discribo']\n",
      "Sentence 9 : ['unde', 'pietas', 'religio']\n",
      "Sentence -1 : ['cicero', 'thos', 'fero', 'thos', 'pagus']\n",
      "Sentence -2 : ['discedo', 'somnus', 'solvo']\n",
      "Sentence -3 : ['animus', 'corpus', 'voluptas', 'dedo', 'quasi', 'minister', 'praebeo', 'impello', 'libido', 'voluptas', 'oboedio', 'deus', 'homo', 'juro', 'uiolauerunt', 'corpus', 'elabor', 'circos', 'terra', 'voluto', 'locus', 'nitor', 'multa', 'exagito', 'saeculum', 'revorto']\n",
      "Sentence -4 : ['bonus', 'cura', 'salus', 'patria', 'agito', 'exercito', 'animus', 'velox', 'sedo', 'domus', 'suo', 'peruolabit', 'ocior', 'facio', 'jam', 'includo', 'corpus', 'emineo', 'foro', 'extro', 'contemplo', 'magnus', 'corpus', 'abstraho']\n",
      "Sentence -5 : ['exerceo', 'bonus', 'res']\n",
      "Sentence -6 : ['edo', 'omne', 'moveo', 'nato', 'certus', 'edo', 'aeternus', 'edo']\n",
      "Sentence -7 : ['inanimus', 'edo', 'omne', 'pello', 'agito', 'externus', 'edo', 'animal', 'moveo', 'cieo', 'internus', 'suo', 'edo', 'proprius', 'natura', 'animus', 'volo']\n",
      "Sentence -8 : ['pateo', 'aeternus', 'edo', 'moveo', 'queo', 'edo', 'natura', 'animus', 'edo', 'tribuo', 'nego']\n",
      "Sentence -9 : ['nasco', 'morior', 'vel', 'concaedes', 'omne', 'caelum', 'omne', 'natura', 'consisto', 'necesse', 'edo', 'vis', 'ullus', 'nanciscor', 'primus', 'impello', 'moveo']\n"
     ]
    }
   ],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "from cltk.stop.latin.stops import STOPS_LIST\n",
    "from string import digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "jv_replacer = JVReplacer()\n",
    "\n",
    "def sentence_cleaner(sentence):\n",
    "    def remove_word(word):\n",
    "        if word in [\".\",\",\",\"-\",\";\",\":\",\"?\",\"&\",\"!\",\"(\",\")\",\"lt\",\"gt\",'\"']:\n",
    "            return True\n",
    "        if \"&\" in word:\n",
    "            return True\n",
    "        if word in STOPS_LIST +[\"que\"] + [\"ne\"] + [\"library\"] + [\"classics\"]:\n",
    "            return True\n",
    "        if word == \"\":\n",
    "            return True\n",
    "        if len(word) == 1:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def lemmatize_word(word):\n",
    "        w = word.lower()\n",
    "        w = w.replace(\"'\",\"\")\n",
    "        w = w.replace('\"','')\n",
    "        w = w.replace(\".\",\"\")\n",
    "        w = jv_replacer.replace(w)\n",
    "        l = lemmatizer.lemmatize(w)\n",
    "        return l\n",
    "    \n",
    "    sent = []\n",
    "    for wd in sentence:\n",
    "        sent += lemmatize_word(wd)\n",
    "    \n",
    "    sent_no_nums = [wd.translate(remove_digits) for wd in sent]\n",
    "\n",
    "    return [wd for wd in sent_no_nums if not remove_word(wd)]\n",
    "\n",
    "\n",
    "\n",
    "sentences = [s for s in [sentence_cleaner(sent) for sent in drp_sents] if len(s) > 0]\n",
    "for i in range(10):\n",
    "    print(\"Sentence\",i,\":\",sentences[i])\n",
    "for i in range(1,10):\n",
    "    print(\"Sentence\",-i,\":\",sentences[-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text corpus contains 12,780 tokens.\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The text corpus contains {0:,} tokens.\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Try a Word2Vec Model\n",
    "\n",
    "We will now attempt a Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "params={\"size\": 100,\n",
    "        \"alpha\":0.025, \n",
    "        \"window\":5, \n",
    "        \"min_count\":0, \n",
    "        \"max_vocab_size\":None, \n",
    "        \"sample\":0.001, \n",
    "        \"seed\":1, \n",
    "        \"workers\":multiprocessing.cpu_count(), \n",
    "        \"min_alpha\":0.0001, \n",
    "        \"sg\":0, \n",
    "        \"hs\":0, \n",
    "        \"negative\":5, \n",
    "        \"cbow_mean\":1, \n",
    "        \"hashfxn\":hash, \n",
    "        \"iter\":5, \n",
    "        \"null_word\":0, \n",
    "        \"trim_rule\":None, \n",
    "        \"sorted_vocab\":1, \n",
    "        \"batch_words\":10000, \n",
    "        \"compute_loss\":False, \n",
    "        \"callbacks\":()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "drp2vec = w2v.Word2Vec(\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "drp2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 2813\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(drp2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113750, 1278000)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drp2vec.train(sentences,total_examples=drp2vec.corpus_count,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "\n",
    "drp2vec.save(os.path.join(\"trained\", \"drp2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drp2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"drp2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marccanby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn import manifold\n",
    "tsne = manifold.TSNE(n_components=2, random_state=0)\n",
    "all_word_vectors_matrix = drp2vec.wv.syn0\n",
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[drp2vec.wv.vocab[word].index])\n",
    "            for word in drp2vec.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(points.shape[0]):\n",
    "    if points['word'][i] == 'lex':\n",
    "        print (points.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_context(\"poster\")\n",
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(7,13), y_bounds=(-5,5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
